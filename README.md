# **FaceSeg**
> [!Note]
> The [next version](https://github.com/Mikzarjr/Ultimate-Segmentation) of `CLIP-DINO-SAM` combination will come out soon!üìÜ

> [!Tip]
> üìÑ Paper with detailed explanation of the structure of the combination of `CLIP-DINO-SAM` models: [PDF](https://pdf.com)
>
> :octocat: Github with detailed workflow of labelling data with `CLIP-DINO-SAM` for `YOLO`: [Github]([(https://pdf.com)](https://github.com/Mikzarjr/Ultimate-Segmentation))

# üëÄ Example Output
Here are example predictions of YOLO model segmenting parts of face after being trained on an auto-labeled dataset using `CLIP-DINO-SAM`

<div align="center">
  <p>
    <a align="center" href="">
      <img
        width="400"
        src="https://github.com/Mikzarjr/FaceSegmentation/blob/main/docks/demo_media/CDS_Output_1.jpeg"
        alt="CDS Output 1"
      >
    </a>
    <a align="center" href="https://github.com/Mikzarjr/FaceSegmentation/blob/main/docks/demo_media/CDS_Output_2.jpeg" target="_blank">
      <img
        width="400"
        src="https://github.com/Mikzarjr/FaceSegmentation/blob/main/docks/demo_media/CDS_Output_2.jpeg"
        alt="CDS Output 2"
      >
    </a>
  </p>
</div>

# üìö Basic Concepts
`CLIP-DINO-SAM` combination is a **Huge** module that works relatively **not quickly** as it requires relatively **Big** ammounts of GPU. So i will show you a detailed workthroug for only two images to save your time on waiting for the results and my time on writing this tutorial. For the most curious ones i will leave a complete workthrough for training on custom face dataset. Enjoy üéâ


#
# üíø Installation
- ### Clone repo
  ```bash
  git clone https://github.com/Mikzarjr/FaceSegmentation
  ```


- ### Locate to working directory
  ___Python___:
  
  ```python
    import os
    HOME = os.getcwd()
    os.chdir(os.path.join(HOME, "FaceSegmentation"))
    
    print("Now in", os.getcwd())
  ```
  _or __Bash___:
  
  ```bash
    HOME=$(pwd)
    cd "${HOME}/FaceSegmentation"
    
    echo "Now in $(pwd)"
  ```

- ### Install requirements:

  ```bash
  pip install -r requirements.txt
  ```

# üöÄ Quickstart
<details>
  
</details>

# üìë Workthrough
## Segmentation with CLIP-DINO-SAM only üé®
<details>

### Import dependencies
```python
from Pipeline.Config import *
from Pipeline.Segmentation import FaceSeg
```

### Choose image to test the framework 
sample images are located in FaceSeg/TestImages
```python
image_path = f"{IMGS_DIR}/img1.jpeg"
```

### Run the following cell to get segmentation masks
Main segmentation mask is located in /segmentation/combined_masks

All separate masks are located in /segmentation/split_masks

```python
S = FaceSeg(image_path)
S.Segment
```
</details>

## Annotations for training YOLO üìù
<details>
  
### Create COCO.json annotations
```python
from Pipeline.Annotator import CreateJson
```
```python
image_path = "/content/segmentation/img1/img1.jpg"
```
```python
A = CreateJson(image_path)
A.CreateJsonAnnotation()
A.CheckJson()
```
Output will be in `COCO_DIR` named `COCO.json`

### Convert COCO.json annotations to YOLOv8 txt annotatoins
```python
from Pipeline.Converter import COCO-to-YOLO
```
```python
json_path = f"{COCO_DIR}/COCO.json"
```
```python
C = ConvertCtY(image_path)
C.Convert()
```
Output will be in `YOLO_DIR` named `YOLO.json`
</details>






